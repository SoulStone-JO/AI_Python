[toc]

<a name = "mao lianjie"></a>
# 线性回归
---
## 最小二乘法

   对于一元线性回归模型,假设从总体中获取了n组观察：
```math
(x_1,y_1),(x_2,y_2),(x_3,y_3)...(x_n,y_n)
```
对于平面中的这n个点，可以使用无数条曲线来拟合。要求样本回归函数尽可能好地拟合这组值。综合起来看，这条直线处于样本数据的中心位置最合理。选择最佳拟合曲线的标准可以确定为：使总的拟合误差（即总残差）达到最小。有以下三个标准可以选择：

（1）用“残差和最小”确定直线位置是一个途径。但很快发现计算“残差和”存在相互抵消的问题。  
（2）用“残差绝对值和最小”确定直线位置也是一个途径。但绝对值的计算比较麻烦。  
（3）最小二乘法的原则是以“残差平方和最小”确定直线位置。用最小二乘法除了计算比较方便外，得到的估计量还具有优良特性。这种方法对异常值非常敏感。


## 最大似然估计
最大似然估计指的是；在已知一系列的采样

```math
{\displaystyle X_{1},X_{2},\ldots ,X_{n}} 
```

和分布（有时是假设）的情况下；求取分布的参数。

理解公式：**L(θ|x)=P(x|θ)**
从右边来看，是已知一系列参数；例如已知正态分布中的均值和方差，求解某个样本的取值范围。从左边来看，是已经得到一些样本，探究在可能的分布下，这些样本最有可能发生的参数，即均值和方差是多少。综合来看，左边同右边，只是同一个方程，自变量和因变量的选择不同。所以他们在数值上是相等的。

### 例：正态分布
以世界上最常见的分布来探讨最大似然估计。  
假设有一组误差，其满足于正态分布，通过采样；已经获取了一系列数据。正态分布满足以下方程：

```math
f(x) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})
```
通过一定的变换使误差**ϵ**满足均值为0的正态分布，由此可得

```math
f(\epsilon) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{\epsilon^2}{2\sigma^2})
```
对于，以下一系列采样，

```math
\epsilon_1,\epsilon_2,\epsilon_3,\epsilon_4,...,\epsilon_m
```
且误差满足;

```math
y_i = \theta^Tx_i + \epsilon_i
```

将ε代入正态分布公式；可得

```math
f(\epsilon) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y_i - \theta^Tx_i)^2}{2\sigma^2})
```

以及似然函数在数值上与概率相等的性质，即

```math
L(\theta|x_i;y_i) = P(y_i|x_i;\theta)
```
可得到

```math
L(\theta|\epsilon_1,\epsilon_2,\epsilon_3,\epsilon_4,...,\epsilon_n) = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y_i - \theta^Tx_i)^2}{2\sigma^2})
```
要求满足样本发生可能性最大的参数θ，即求L(θ)达到最大值时，θ的取值。
此时；令

```math
\begin{array}{l}
\quad l(\theta)  \\
= logL(\theta) \\
=\sum_{i=1}^{m}log\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y_i - \theta^Tx_i)^2}{2\sigma^2}) \\
=mlog\frac{1}{\sqrt{2\pi}}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y_i - \theta^Tx_i)^2 \\
\end{array}
```
其中，令

```math
J(\theta) =\frac{1}{2} \sum_{i=1}^{m}(y_i-\theta^Tx_i)^2
```
我们可知，**Y**为m维列向量；**X**为(m,n)的列向量，θ为n维列向量。J(θ)具有矩阵形式。即:

```math
J(\theta) = (Y - X\theta)^T(Y - X\theta)
```
要求L(θ)何时取得最大值；即求l(θ)的导数何时为0；即J(θ)的导数合适为0；根据矩阵求导公式；

```math
\frac{\partial{A\theta}}{\partial\theta} = A^T
```

```math
\frac{\partial{\theta^TA}}{\partial\theta} = A
```

```math
\frac{\partial{\theta^TA\theta}}{\partial\theta} = 2A\theta
```
化简**J(θ)**

```math

\begin{array}{l}
\quad J(\theta)\\
=\frac{1}{2}(\theta^TX^T-Y^T)(X\theta-Y)\\
=\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^TY-Y^TX\theta+Y^TY)\\
\end{array}
```
应用矩阵求导公式对J(θ)求导:
```math
\begin{array}{l}
\quad \frac{\partial{L(\theta)}}{\partial\theta} \\
= \frac{1}{2}(2X^TX\theta-X^TY-X^TY)\\
=X^TX\theta-X^TY=0\\
\end{array}
```
可得:

```math
\theta = (X^TX)^{-1}X^TY
```
通过这个过程，我们就可以获得参数θ的值。

### 凸函数的判别
J(θ)存在极值的前提是J(θ)是凸函数，为判定J(θ)的凹凸性，引入黑塞矩阵。黑塞矩阵是一个多元函数的二阶
偏导数构成的方阵，描述了函数的局部曲率。
此处，J(θ)的黑塞矩阵如下：

```math
\frac{\partial^2{L(\theta)}}{\partial\theta^2} = X^TX
```
故损失函数J(θ)大于等于0恒成立，故解析解恒存在。

## 利用PYTHON实现线性回归

### PYTHON相关知识
#### 随机生成一个均匀分布的数组
可以利用numpy.rand(dn)方法或者numpy.random.random((m,n))方法

```
import numpy as np
N = np.random.rand(5,3)
print(N)
# result as follows
[[0.61318029 0.58931051 0.81204214]
 [0.56210957 0.60609129 0.61073364]
 [0.40561645 0.71289917 0.74933713]
 [0.72668731 0.46699556 0.67433632]
 [0.72984035 0.49029903 0.30881497]]

```

```
import numpy as np
N = np.random.random((5, 3))
print(N)
# result as follows
[[0.99516711 0.23143644 0.92126703]
 [0.67434352 0.58450435 0.8893598 ]
 [0.37360603 0.05451519 0.52547943]
 [0.90395957 0.08936911 0.25193238]
 [0.83702534 0.44734759 0.30165836]]
```
#### 整合两个矩阵

np.r_[]是按列连接两个矩阵，就是把两矩阵上下相加，要求列数相等。

np.c_[]是按行连接两个矩阵，就是把两矩阵左右相加，要求行数相等。

```
import numpy as np

M = np.ones((3, 3))
N = np.zeros((3, 3))
print(N)
print(M)
print(np.c_[N, M])
print(np.r_[N, M])

[[0. 0. 0.] # N矩阵
 [0. 0. 0.]
 [0. 0. 0.]]
 
[[1. 1. 1.] # M矩阵
 [1. 1. 1.]
 [1. 1. 1.]]
 
[[0. 0. 0. 1. 1. 1.]   # np.c_[N, M]
 [0. 0. 0. 1. 1. 1.]
 [0. 0. 0. 1. 1. 1.]]
 
[[0. 0. 0.]    # np.r_[N, M]
 [0. 0. 0.]
 [0. 0. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [1. 1. 1.]]
```
#### 生成正态分布的样本
numpy.random.randn(M,N)是从标准正态分布中返回一个或多个样本值,并生成矩阵。 

```
import numpy as np

M = np.random.randn(3, 3)
print(M)

# result as follows
[[ 0.01154088  1.27785341 -0.95545444]
 [ 0.56113882  0.67005698 -0.1543482 ]
 [ 0.68480804 -1.49940067  0.61181177]]
```

### PYTHON代码


```
import numpy as np
import matplotlib.pyplot as plt

# 生成100个[0,2)之间均匀分布的随机数。
X_1 = 2 * np.random.rand(100, 1)
# print(len(X))
# print(X)

# Y = X0(全为1) * \theta_0 + X1 * \theta_1
# 设置X0，并整合X0和X1
X_0 = np.ones((100, 1))
X = np.c_[X_0, X_1]
# print(X)

# 人为设置结果Y，并建立满足正态分布的误差
Y = 5 * X_0 + 4 * X_1 + np.random.randn(100, 1)
# print(Y)

# 解析解求解\theta
theta_best = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)
print(theta_best, "\\theta")

# 创建测试集里面的X1
X_new = np.array([[0], [2]])
X_new_b = np.c_[(np.ones((2, 1))), X_new]
print(X_new_b)
y_predict = X_new_b.dot(theta_best)
print(y_predict, "\\y_predict")

plt.plot(X_new, y_predict, 'r-')
plt.plot(X_1, Y, 'b.')
plt.axis([0, 2, 0, 15])
plt.show()

# results as follows
[[4.97002309]
 [4.00500511]] \theta
[[1. 0.]
 [1. 2.]]
[[ 4.97002309]
 [12.98003331]] \y_predict
```
### sklearn.linear_model库代码

```
from sklearn.linear_model import LinearRegression
import numpy as np
X = 2 * np.random.rand(100, 1)
# 整合X0和X1
X_b = np.c_[np.ones((100, 1)), X]
y = 5 + 4 * X + np.random.randn(100, 1)
# 新建模型对象
model = LinearRegression()
# 训练
model.fit(X_b, y)
# 训练的结果
print(model.intercept_)      # 截距
print(model.coef_)           # 系数

# 预测
X_new = np.array([[0], [2]])
X_new_b = np.c_[(np.ones((2, 1))), X_new]
y_pre = model.predict(X_new_b)
print(y_pre)

```

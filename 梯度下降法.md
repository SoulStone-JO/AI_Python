[toc]

# 梯度下降法

## 梯度下降法思想
相比与解析法来说，公式中的矩阵计算随着特征量的增加，计算复杂度也将大大增加，为减少计算时间，引入梯度下降法。
> 首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

首先我们要初始化一个随机的参数值θ，函数的梯度方向是函数增长速度最快的方向，为了寻找一个较为合适的θ使J(θ)的值最低，我们要沿着梯度的逆方向，迭代地寻找θ。
计算各个维度上的梯度：

```math
\frac{\partial J(\theta)}{\partial \theta_i} = \frac{\partial}{\partial \theta_i}(h_\theta(X)-Y)^2
= (h_\theta(X)-Y)x_i
```
因此，在迭代过程中；

```math
\theta += \theta + \alpha\sum_{i=1}^m(y_i - h_\theta(x_i))x_i
```
直到θ收敛，即上一次的迭代误差与这次的迭代误差均方差在极小的范围内，即可完成迭代。

## 梯度下降法过程


### 批量量梯度下降法（Batch gradient descent）
#### PYTHON 实现代码

```
#!/usr/bin/python
# coding=utf-8
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# 构造训练数据
x = np.arange(0., 10., 0.2)
m = len(x)  # 训练数据点数目
print("样本数量为：", m)
x0 = np.full(m, 1.0)
# print(x)
# print(x0)

input_data = np.vstack([x0, x]).T  # 将偏置b作为权向量的第一个分量
print("\n", "样本X = ", input_data)
target_data = 2 * x + 5 + np.random.randn(m)
print("\n", "样本Y = ", target_data)

# 两种终止条件
loop_max = 10000  # 最大迭代次数(防止死循环)
epsilon = 1e-5

# 初始化权值
np.random.seed(0)
theta = np.random.randn(2)
print("\n", "初始参数为：", theta)

alpha = 0.000001  # 步长(注意取值过大会导致振荡即不收敛,过小收敛速度变慢)
diff = 0.
error = np.zeros(2)
count = 0  # 循环次数
finish = 0  # 终止标志

while count < loop_max:
    count += 1

    # 标准梯度下降是在权值更新前对所有样例汇总误差，而随机梯度下降的权值是通过考查某个训练样例来更新的
    # 在标准梯度下降中，权值更新的每一步对多个样例求和，需要更多的计算
    sum_m = np.zeros(2)
    for i in range(m):
        dif = (np.dot(theta, input_data[i]) - target_data[i]) * input_data[i]
        sum_m = sum_m + dif  # 当alpha取值过大时,sum_m会在迭代过程中会溢出

    theta = theta - alpha * sum_m  # 注意步长alpha的取值,过大会导致振荡
    # theta = theta - 0.005 * sum_m      # alpha取0.005时产生振荡,需要将alpha调小

    # 判断是否已收敛
    if np.linalg.norm(theta - error) < epsilon:
        finish = 1
        break
    else:
        error = theta
    print('loop count = %d' % count, '\tw:', theta)
print('loop count = %d' % count, '\tw:', theta)

# check with scipy linear regression
slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x, target_data)
print('intercept = %s slope = %s' % (intercept, slope))

plt.plot(x, target_data, 'g*')
plt.plot(x, theta[1] * x + theta[0], 'r')
plt.show()
print(count)

```

### 随机梯度下降法（Stochastic Gradient Descent）

### 小批量梯度下降法（Mini-Batch Gradient Descent）

### Momentum梯度下降法

### NAG梯度下降法

## 归一化方法（Normalization）
把每列数据变换到差不多的数量级
预处理方法
sklearn.preprocessing
归一化是只对x操作

不同特征之间的数量级差异过大
- 最大值最小值归一化
- 均值归一化
- 方差归一化
